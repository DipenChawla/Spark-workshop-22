{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark isn't on sys.path by default, but that doesn't mean it can't be used as a regular library. You can address this by either symlinking pyspark into your site-packages, or adding pyspark to sys.path at runtime. findspark does the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession introduced in version 2.0 and and is an entry point to underlying Spark functionality in order to programmatically create Spark RDD, DataFrame and DataSet. It’s object spark is default available in spark-shell and it can be created programmatically using SparkSession builder pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Initialise the Spark Session\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Generate fake data on the driver\n",
    "mylist = [\n",
    "    [\"Julien\", 67], \n",
    "    [\"Ιουλιανός\", 32], \n",
    "    [\"Юлиан\", 89],\n",
    "    [\"尤利安\", 40]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Spark “driver” is an application that creates a SparkContext for executing one or more jobs in the Spark cluster. It allows your Spark/PySpark application to access Spark Cluster with the help of Resource Manager.\n",
    "\n",
    "When you create a SparkSession object, SparkContext is also created and can be retrieved using spark.sparkContext. SparkContext will be created only once for an application; even if you try to create another SparkContext, it still returns existing SparkContext."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role of a Spark Context\n",
    "\n",
    "Spark Context is the entry point. Like a key to your car.\n",
    "\n",
    "Functions -\n",
    "* Tells Sparks how to access a cluster\n",
    "* Allocates Executors\n",
    "* The context, living in your driver program, coordinates sets of processes on the cluster to run your application.\n",
    "* The context keeps track of live executors by sending heartbeat messages periodically.\n",
    "* Finally, the context may perform dynamic resource allocation if the cluster manager permits. This increases cluster utilization in shared environments by proper scheduling of multiple applications according to their resource demands.\n",
    "\n",
    "#### Scenario 1:\n",
    "\n",
    "In the sense, if you want to compute a complex aggregation on spark, you need to distribute the task in the cluster.\n",
    "\n",
    "Spark context is the gateway to a Spark Cluster.\n",
    "\n",
    "#### Scenario 2:\n",
    "\n",
    "If you have a dataset ( simple CSV/TXT file) and want computations on this data, you want all the worker nodes to have access to this data. Use the spark context to broadcast this file to all the nodes.\n",
    "\n",
    "It allows your Spark Application to access Spark Cluster with the help of Resource Manager. The resource manager can be one of these three- Spark Standalone, YARN, Apache Mesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 What are RDDs?\n",
    "\n",
    "* RDDs stands for Resilient Distributed Dataset.\n",
    "\n",
    "* RDD is the core abstraction in Apache Spark. (In case you are thinking, What does abstraction mean in programming? then go through this excellent expalnation over stackoverflow - Abstraction\n",
    "\n",
    "* An RDD is simply a immutable distributed collection of elements.\n",
    "\n",
    "* The name captures two important properties:\n",
    "\n",
    "    - Resilient means that we must be able to withstand failures and complete an ongoing computation.\n",
    "Distributed means that we must account for multiple machines having a subset of data. Formally, RDD is a read-only, partitioned collection of records\n",
    "    - In Spark all work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result.\n",
    "\n",
    "* The data inside a spark application is read into the form of RDDs and then Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them.\n",
    "\n",
    "* RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\n",
    "\n",
    "## 1.2 Creating RDDs\n",
    "Spark provides two ways to create RDDs:\n",
    "\n",
    "* Parallelizing a collection in your driver program.\n",
    "* Loading an external dataset\n",
    "\n",
    "\n",
    "#### Parallelizing a collection in your driver program:\n",
    "\n",
    "Create RDDs using parallelize() method on existing iterable or collection in your driver program. We will be doing this in our first program\n",
    "\n",
    "#### Loading an external dataset\n",
    "Two methods: These methods takes an URI for the file (either a local path on the machine, or a hdfs://, s3n://, etc URI).\n",
    "\n",
    "- `sc.textFile()`: Creates a RDD with each line as an element. \n",
    "- `sc.wholeTextFiles()` :  Creates a PairRDD with the key being the file name with a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Distribute it over the network\n",
    "rdd = spark.sparkContext.parallelize(mylist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Operations on RDDs\n",
    "\n",
    "From the Spark Programming Guide:\n",
    "\n",
    "> RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset.\n",
    "\n",
    "### Transformation on RDDs\n",
    "Transformations are kind of operations which will transform your RDD data from one form to another. And when you apply this operation on any RDD, you will get a new RDD with transformed data\n",
    "\n",
    "Operations like map, filter, flatMap are transformations.\n",
    "\n",
    "> Note: when you apply the transformation on any RDD it will not perform the operation immediately. It will create a DAG(Directed Acyclic Graph) using the applied operation, source RDD and function used for transformation. And it will keep on building this graph using the references till you apply any action operation on the last lined up RDD. That is why the transformation in Spark are lazy.\n",
    "\n",
    "### Actions on RDDs\n",
    "This kind of operation will also give you another RDD but this operation will trigger all the lined up transformation on the base RDD (or in the DAG) and than execute the action operation on the last RDD.\n",
    "Operations like collect, take, count, first, saveAsTextFile are actions\n",
    "Let's have a look to data in your RDD currently looks like. We will use take() method for this:\n",
    "\n",
    "take(n):\n",
    "\n",
    "The action take(n) returns n number of elements from RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Julien', 67], ['Ιουλιανός', 32], ['Юлиан', 89]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean age is 57.0\n"
     ]
    }
   ],
   "source": [
    "# (4) Return the mean of ages:\n",
    "meanage = rdd.map(lambda x: x[1]).mean()\n",
    "print(\"Mean age is {}\".format(meanage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) Return person whose age is below 60\n",
    "belowthreshold = rdd\\\n",
    "    .filter(lambda x: x[1] < 60)\\\n",
    "    .map(lambda x: x[0])\\\n",
    "    .collect()\n",
    "print(\"{} is/are below 60\".format(belowthreshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) Go from RDD to DataFrame\n",
    "df = rdd.toDF([\"Name\", \"Age\"])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
