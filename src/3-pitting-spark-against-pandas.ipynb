{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL and DataFrames\n",
    "\n",
    "### Spark SQL vs RDDs\n",
    "Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed.\n",
    "\n",
    "### Spark DataFrames \n",
    "A DataFrame is a two-dimensional labeled data structure with columns of potentially different types. You can think of a DataFrame like a spreadsheet, a SQL table, or a dictionary of series objects\n",
    "\n",
    "### Advantages of using DataFrame API over RDDs\n",
    "\n",
    "1. In-built Optimization: DataFrame uses Catalyst engine which has an in-built execution optimization that improves the data processing performance significantly. When an action is called on a DataFrame, the Catalyst engine analyzes the code and resolves the references. Then, it creates a logical plan. After that, the created logical plan gets translated into an optimized physical plan. Finally, this physical plan gets executed on the cluster.\n",
    "2. Hive compatible: The DataFrame is fully compatible with Hive query language. We can access all hive data, queries, UDFs, etc using Spark SQL from hive MetaStore and can execute queries against these hive databases.\n",
    "3. Structured, semi-structured, and highly structured data support: DataFrame APIs supports manipulation of all kind of data from structured data files to semi-structured data files and highly structured parquet files.\n",
    "4. Multi-language support: DataFrame APIs are available in Python, R, Scala, and Java.\n",
    "5. Schema support: We can define a schema manually or we can read a schema from a data source which defines the column names and their data types.\n",
    "\n",
    "\n",
    "### Comparing Spark DF API vs RDDs\n",
    "| Feature                | RDD | DataFrame |\n",
    "|------------------------|-----|-----------|\n",
    "| Abstraction Level      | Low | High      |\n",
    "| Immutable              | Yes | Yes       |\n",
    "| Fault tolerant         | Yes | Yes       |\n",
    "| TypeSafe               | Yes | No        |\n",
    "| Schema                 | No  | Yes       |\n",
    "| Execution Optimization | No  | Yes       |\n",
    "\n",
    "\n",
    "### Pandas DataFrame vs Spark DataFrame\n",
    "\n",
    "| Spark DataFrame                                                                                                  | Pandas DataFrame                                                                                                            |   |\n",
    "|------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------|---|\n",
    "| Spark DataFrame supports parallelization.                                                                        | Pandas DataFrame does not support parallelization.                                                                          |   |\n",
    "| Spark DataFrame has Multiple Nodes.                                                                              | Pandas DataFrame has a Single  Node.                                                                                        |   |\n",
    "| It follows Lazy Execution which means that a task is not executed until an action is performed.                  | It follows Eager Execution, which means task is executed immediately.                                                       |   |\n",
    "| Spark DataFrame is Immutable.                                                                                    | Pandas DataFrame is Mutable.                                                                                                |   |\n",
    "| Complex operations are difficult to perform as compared to Pandas DataFrame.                                     | Complex operations are easier to perform as compared to Spark DataFrame.                                                    |   |\n",
    "| Spark DataFrame is distributed and hence processing in the Spark DataFrame is faster for a large amount of data. | Pandas DataFrame is not distributed and hence processing in the Pandas DataFrame will be slower for a large amount of data. |   |\n",
    "| sparkDataFrame.count() returns the number of rows.                                                               | pandasDataFrame.count() returns the number of non NA/null observations for each column.                                     |   |\n",
    "| Spark DataFrames are excellent for building a scalable application.                                              | Pandas DataFrames can‚Äôt be used to build a scalable application.                                                            |   |\n",
    "| Spark DataFrame assures fault tolerance.                                                                         | Pandas DataFrame does not assure fault tolerance. We need to implement our own framework to assure it.                      |   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(344, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üêº pandas \n",
    "pandasdf = pd.read_csv('../data/penguins.csv')\n",
    "pandasdf.shape\n",
    "\n",
    "# üéá PySpark\n",
    "df = spark.read.csv('../data/penguins.csv', header=True, inferSchema=True)\n",
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 344 entries, 0 to 343\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   species            344 non-null    object \n",
      " 1   island             344 non-null    object \n",
      " 2   bill_length_mm     342 non-null    float64\n",
      " 3   bill_depth_mm      342 non-null    float64\n",
      " 4   flipper_length_mm  342 non-null    float64\n",
      " 5   body_mass_g        342 non-null    float64\n",
      " 6   sex                333 non-null    object \n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 18.9+ KB\n",
      "root\n",
      " |-- species: string (nullable = true)\n",
      " |-- island: string (nullable = true)\n",
      " |-- bill_length_mm: double (nullable = true)\n",
      " |-- bill_depth_mm: double (nullable = true)\n",
      " |-- flipper_length_mm: integer (nullable = true)\n",
      " |-- body_mass_g: integer (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üêº pandas \n",
    "pandasdf.info()\n",
    "# üéá PySpark\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------+-------------+-----------------+-----------+------+\n",
      "|species|   island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|\n",
      "+-------+---------+--------------+-------------+-----------------+-----------+------+\n",
      "| Adelie|Torgersen|          39.1|         18.7|              181|       3750|  MALE|\n",
      "| Adelie|Torgersen|          39.5|         17.4|              186|       3800|FEMALE|\n",
      "| Adelie|Torgersen|          40.3|         18.0|              195|       3250|FEMALE|\n",
      "| Adelie|Torgersen|          null|         null|             null|       null|  null|\n",
      "| Adelie|Torgersen|          36.7|         19.3|              193|       3450|FEMALE|\n",
      "+-------+---------+--------------+-------------+-----------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üêº pandas \n",
    "df.head()\n",
    "# üéá PySpark\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|species|   island|\n",
      "+-------+---------+\n",
      "| Adelie|Torgersen|\n",
      "| Adelie|Torgersen|\n",
      "| Adelie|Torgersen|\n",
      "+-------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üêº pandas \n",
    "pandasdf[['species', 'island']].head(3)\n",
    "# üéá PySpark\n",
    "df[['species', 'island']].show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------+-------------+-----------------+-----------+------+\n",
      "|species|island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+------+\n",
      "| Gentoo|Biscoe|          46.1|         13.2|              211|       4500|FEMALE|\n",
      "| Gentoo|Biscoe|          50.0|         16.3|              230|       5700|  MALE|\n",
      "| Gentoo|Biscoe|          48.7|         14.1|              210|       4450|FEMALE|\n",
      "| Gentoo|Biscoe|          50.0|         15.2|              218|       5700|  MALE|\n",
      "| Gentoo|Biscoe|          47.6|         14.5|              215|       5400|  MALE|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üêº pandas \n",
    "pandasdf[pandasdf['species']=='Gentoo'].head()\n",
    "# üéá PySpark\n",
    "df[df['species']=='Gentoo'].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>46.1</td>\n",
       "      <td>13.2</td>\n",
       "      <td>211.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>50.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>230.0</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>48.7</td>\n",
       "      <td>14.1</td>\n",
       "      <td>210.0</td>\n",
       "      <td>4450.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>50.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>218.0</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>47.6</td>\n",
       "      <td>14.5</td>\n",
       "      <td>215.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    species  island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "220  Gentoo  Biscoe            46.1           13.2              211.0   \n",
       "221  Gentoo  Biscoe            50.0           16.3              230.0   \n",
       "222  Gentoo  Biscoe            48.7           14.1              210.0   \n",
       "223  Gentoo  Biscoe            50.0           15.2              218.0   \n",
       "224  Gentoo  Biscoe            47.6           14.5              215.0   \n",
       "\n",
       "     body_mass_g     sex  \n",
       "220       4500.0  FEMALE  \n",
       "221       5700.0    MALE  \n",
       "222       4450.0  FEMALE  \n",
       "223       5700.0    MALE  \n",
       "224       5400.0    MALE  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üêº pandas \n",
    "pandasdf[pandasdf['species'].str.match('G.')].head()\n",
    "pandasdf[(pandasdf['mass']<3400) & (pandasdf['sex']=='Male')].head()\n",
    "# üéá PySpark\n",
    "df[(df['mass']<3400) & (df['sex']=='Male')].show(5)\n",
    "df[df['species'].rlike('G.')].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üêº pandas \n",
    "df.nsmallest(5, 'mass')\n",
    "# üéá PySpark\n",
    "df[df['mass'].isNotNull()].orderBy('mass').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üêº pandas \n",
    "pandasdf.agg({\"flipper\": \"mean\"})\n",
    "# üéá PySpark\n",
    "df.agg({'flipper': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also substitute .mean() with .avg() as well. In other words, we can use df.groupBy(‚Äòspecies‚Äô).avg().show().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üêº pandas \n",
    "df['species'].unique()\n",
    "# üéá PySpark\n",
    "df.select('species').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üêº pandas \n",
    "df.groupby('species')['mass'].mean()\n",
    "# üéá PySpark\n",
    "df.groupBy('species').agg({'mass': 'mean'}).show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
